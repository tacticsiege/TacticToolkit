{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TacticToolkit Introduction\n",
    "\n",
    "TacticToolkit is a codebase to assist with machine learning and natural language processing. We build on top of sklearn, tensorflow, keras, nltk, spaCy and other popular libraries.  The TacticToolkit will help throughout; from data acquisition to preprocessing to training to inference.  \n",
    "\n",
    "| Modules       | Description                                          |\n",
    "|---------------|------------------------------------------------------|\n",
    "| corpus        | Load and work with text corpora                      |\n",
    "| data          | Data generation and common data functions            |\n",
    "| plotting      | Predefined and customizable plots                    |\n",
    "| preprocessing | Transform and clean data in preparation for training |\n",
    "| sandbox       | Newer experimental features and references           |\n",
    "| text          | Text manipulation and processing                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# until we can install, add parent dir to path so ttk is found\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with some text\n",
    "The ttk.text module includes classes and functions to make working with text easier.  These are meant to supplement existing nltk and spaCy text processing, and often work in conjunction with these libraries.  Below is an overview of some of the major components. We'll explore these objects with some simple text now.\n",
    "\n",
    "| Class           | Purpose                                                                |\n",
    "|-----------------|------------------------------------------------------------------------|\n",
    "| Normalizer      | Normalizes text by formatting, stemming and substitution               |\n",
    "| Tokenizer       | High level tokenizer, provides word, sentence and paragraph tokenizers |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple text normalization\n",
    "# apply individually\n",
    "# apply to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple text tokenization\n",
    "# harder text tokenization\n",
    "# sentence tokenization\n",
    "# paragraph tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpii? Corpuses? Corpora!\n",
    "\n",
    "The ttk.corpus module builds on the nltk.corpus model, adding new corpus readers and corpus processing objects. It also includes loading functions for the corpora included with ttk, which will download the content from github as needed. \n",
    "\n",
    "We'll use the Dated Headline corpus included with ttk. This corpus was created using ttk, and is maintained in a complimentary github project, TacticCorpora (https://github.com/tacticsiege/TacticCorpora).\n",
    "\n",
    "First, a quick look at the corpus module's major classes and functions.\n",
    "\n",
    "| Class                        | Purpose                                                                          |\n",
    "|------------------------------|----------------------------------------------------------------------------------|\n",
    "|CategorizedDatedCorpusReader  |Extends nltk's CategorizedPlainTextCorpusReader to include a second category, Date|           \n",
    "|CategorizedDatedCorpusReporter|Summarizes corpora. Filterable, and output can be str, list or DataFrame          |\n",
    "\n",
    "\n",
    "| Function                             | Purpose                                                                 |\n",
    "|--------------------------------------|-------------------------------------------------------------------------|\n",
    "| load_headline_corpus(with_date=True) | Loads Categorized or CategorizedDated CorpusReader from headline data   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from: S:\\git\\tacticsiege\\tactictoolkit\\ttk\\..\\env\\corpus\\dated\\2017_08_22\\corpus\n",
      "Corpus loaded.\n"
     ]
    }
   ],
   "source": [
    "from ttk.corpus import load_headline_corpus\n",
    "\n",
    "# load the dated corpus. This will attempt to download the corpus from github if it is not present locally.\n",
    "corpus = load_headline_corpus(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 categories\n",
      "CBS News\n",
      "The Atlantic\n",
      "Slate\n",
      "Associated Press\n",
      "The Guardian\n",
      "Economist\n",
      "Fox News\n",
      "Russia Today\n",
      "Reuters\n",
      "Weekly Standard\n",
      "The Independent\n",
      "Al Jazeera\n",
      "NPR\n",
      "BBC\n",
      "Boston Globe\n",
      "CNBC\n",
      "Business Insider\n",
      "Wall Street Journal\n",
      "Washington Post\n",
      "Huffington Post\n",
      "ABC News\n",
      "Evening Standard\n",
      "CNN\n",
      "Breitbart\n",
      "New York Times\n",
      "The New Yorker\n"
     ]
    }
   ],
   "source": [
    "# inspect categories\n",
    "print (len(corpus.categories()), 'categories')\n",
    "for cat in corpus.categories():\n",
    "    print (cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 categories\n",
      "CBS News\n",
      "The Atlantic\n",
      "Associated Press\n",
      "The Guardian\n",
      "Fox News\n",
      "Russia Today\n",
      "Reuters\n",
      "Weekly Standard\n",
      "The Independent\n",
      "Al Jazeera\n",
      "NPR\n",
      "BBC\n",
      "Boston Globe\n",
      "CNBC\n",
      "Business Insider\n",
      "Wall Street Journal\n",
      "Washington Post\n",
      "Huffington Post\n",
      "ABC News\n",
      "Evening Standard\n",
      "CNN\n",
      "Breitbart\n",
      "New York Times\n",
      "The New Yorker\n"
     ]
    }
   ],
   "source": [
    "# all main corpus methods allow lists of categories and dates filters\n",
    "d = '2017-08-22'\n",
    "print (len(corpus.categories(dates=[d])), 'categories')\n",
    "for cat in corpus.categories(dates=[d]):\n",
    "    print (cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBS News              88 dates  10412 sentences  113282 words  12288 unique words,  88 files\n",
      "The Atlantic          86 dates   2610 sentences   24953 words   5077 unique words,  86 files\n",
      "Slate                 79 dates   1062 sentences    7899 words   2428 unique words,  79 files\n",
      "Associated Press      88 dates   7711 sentences   83257 words   9989 unique words,  88 files\n",
      "The Guardian          87 dates  15278 sentences  205447 words  19845 unique words,  87 files\n",
      "Economist             45 dates    252 sentences    2304 words   1007 unique words,  45 files\n",
      "Fox News              86 dates  11213 sentences  145350 words  15793 unique words,  86 files\n",
      "Russia Today          88 dates   6463 sentences   92975 words  12692 unique words,  88 files\n",
      "Reuters               88 dates   9462 sentences  117009 words  10973 unique words,  88 files\n",
      "Weekly Standard       80 dates   1161 sentences   10188 words   3077 unique words,  80 files\n",
      "The Independent       87 dates   4584 sentences   72318 words  10048 unique words,  87 files\n",
      "Al Jazeera            87 dates   3164 sentences   29095 words   5565 unique words,  87 files\n",
      "NPR                   88 dates   6626 sentences   79370 words  11101 unique words,  88 files\n",
      "BBC                   88 dates  12198 sentences  121363 words  15447 unique words,  88 files\n",
      "Boston Globe          86 dates    607 sentences    7062 words   2275 unique words,  86 files\n",
      "CNBC                  88 dates  12726 sentences  186294 words  14217 unique words,  88 files\n",
      "Business Insider      86 dates   7949 sentences  122707 words  13074 unique words,  86 files\n",
      "Wall Street Journal   88 dates   3573 sentences   37259 words   6257 unique words,  88 files\n",
      "Washington Post       88 dates  16230 sentences  209588 words  15682 unique words,  88 files\n",
      "Huffington Post       88 dates   3361 sentences   43254 words   6568 unique words,  88 files\n",
      "ABC News              88 dates  15891 sentences  181688 words  14233 unique words,  88 files\n",
      "Evening Standard      84 dates   5522 sentences   87879 words  10763 unique words,  84 files\n",
      "CNN                   88 dates  18383 sentences  176168 words  14749 unique words,  88 files\n",
      "Breitbart             88 dates   6750 sentences   99341 words  11599 unique words,  88 files\n",
      "New York Times        88 dates   6304 sentences   83061 words  10201 unique words,  88 files\n",
      "The New Yorker        80 dates    955 sentences    9504 words   2881 unique words,  80 files\n"
     ]
    }
   ],
   "source": [
    "# use the Corpus Reporters to get summary reports\n",
    "from ttk.corpus import CategorizedDatedCorpusReporter\n",
    "reporter = CategorizedDatedCorpusReporter()\n",
    "\n",
    "# summarize categories\n",
    "print (reporter.category_summary(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2017-08-17', 'categories': 25, 'sentences': 2843, 'words': 36125, 'uniq_words': 7255, 'files': 25}\n",
      "{'date': '2017-08-18', 'categories': 25, 'sentences': 2773, 'words': 34342, 'uniq_words': 6891, 'files': 25}\n",
      "{'date': '2017-08-19', 'categories': 24, 'sentences': 1445, 'words': 17718, 'uniq_words': 4435, 'files': 24}\n"
     ]
    }
   ],
   "source": [
    "# reporters can return str, list or dataframe\n",
    "for s in reporter.date_summary(corpus, dates=['2017-08-17', '2017-08-18', '2017-08-19',], output='list'):\n",
    "    print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>dates</th>\n",
       "      <th>files</th>\n",
       "      <th>sentences</th>\n",
       "      <th>uniq_words</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>18383</td>\n",
       "      <td>14749</td>\n",
       "      <td>176168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NPR</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>6626</td>\n",
       "      <td>11101</td>\n",
       "      <td>79370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNBC</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>12726</td>\n",
       "      <td>14217</td>\n",
       "      <td>186294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BBC</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>12198</td>\n",
       "      <td>15447</td>\n",
       "      <td>121363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  dates  files  sentences  uniq_words   words\n",
       "0      CNN     88     88      18383       14749  176168\n",
       "1      NPR     88     88       6626       11101   79370\n",
       "2     CNBC     88     88      12726       14217  186294\n",
       "3      BBC     88     88      12198       15447  121363"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_frame = reporter.category_summary(corpus, categories=['BBC', 'CNBC', 'CNN', 'NPR',], output='dataframe')\n",
    "cat_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
